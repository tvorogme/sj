{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:56.341291Z",
     "start_time": "2018-02-13T11:20:56.338194Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple, Sequence\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from cytoolz import pipe\n",
    "from collections import Counter\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:56.919774Z",
     "start_time": "2018-02-13T11:20:56.574584Z"
    },
    "code_folding": [
     11,
     15,
     21,
     27,
     34,
     38,
     47,
     51
    ]
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stopwords = nltk.corpus.stopwords.words(\n",
    "    'russian') + nltk.corpus.stopwords.words('english')\n",
    "stopwords += [\n",
    "    'отличный', 'метр', 'наш', 'клиент', 'банка', 'проект', 'литр',\n",
    "    'желательный', 'др', 'самый', 'мочь', 'хороший', 'год', 'чел', 'обязательный'\n",
    "]\n",
    "\n",
    "cache = {}\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def _get_POS(word):\n",
    "    if word not in cache:\n",
    "        cache[word] = morph.parse(word)[0]\n",
    "    return cache[word].tag.POS\n",
    "\n",
    "\n",
    "def normal_form(word):\n",
    "    if word not in cache:\n",
    "        cache[word] = morph.parse(word)[0]\n",
    "    return cache[word].normal_form\n",
    "\n",
    "\n",
    "def is_word_pos_in(word: str, pos: List[str] = None) -> bool:\n",
    "    if not pos:\n",
    "        pos = ['NOUN', \"ADJF\", 'INFN', 'VERB', 'ADJS']\n",
    "\n",
    "    return _get_POS(word) in pos\n",
    "\n",
    "\n",
    "def get_words(text):\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "\n",
    "def nonempty(x):\n",
    "    if isinstance(x, Sequence):\n",
    "        return filter(lambda x: len(x) > 0 and x != ' ', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "helper = {}\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def normalize_skill(skill: str):\n",
    "    parsed = tuple(\n",
    "        pipe(\n",
    "            skill,\n",
    "            lambda x: x.lower(),\n",
    "            remove_numbers,\n",
    "            get_words,\n",
    "        ))\n",
    "\n",
    "    clear_skill = []\n",
    "    dirty_skill = []\n",
    "\n",
    "    # Последнее стоп слово для dirty_skill\n",
    "    last_stopword = None\n",
    "\n",
    "    # Для каждого слова в скилле\n",
    "    for i in parsed:\n",
    "        # Нормализуем слово\n",
    "        word = normal_form(i)\n",
    "\n",
    "        # Если стоп слово - запомним его\n",
    "        if word in nltk.corpus.stopwords.words('russian'):\n",
    "            last_stopword = word\n",
    "\n",
    "            if word == \"без\":\n",
    "                clear_skill.append(word)\n",
    "\n",
    "        # Проверим на часть речи, длинну и стоплова\n",
    "        elif is_word_pos_in(word) and len(word) > 3 and word not in stopwords:\n",
    "\n",
    "            # Если до этого было стоп слово, добавим его в dirty\n",
    "            if last_stopword and len(dirty_skill) > 0:\n",
    "                dirty_skill.append(last_stopword)\n",
    "                last_stopword = None\n",
    "\n",
    "            # Добавим в чистый скилл слово\n",
    "            clear_skill.append(word)\n",
    "\n",
    "            if is_word_pos_in(word, ['NOUN', 'ADJF']):\n",
    "                dirty_skill.append(i)\n",
    "\n",
    "    if len(clear_skill) > 1 and len(clear_skill) < 8:\n",
    "        return clear_skill, dirty_skill\n",
    "\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def split_skill(a):\n",
    "    '''Рассказ о программистах или о жизни -> рассказ о программистах, рассказ о жизни'''\n",
    "    main_skill = a\n",
    "\n",
    "    def repl(x): return x.replace(',', '|').replace(\n",
    "        ' или ', '|').replace(' и ', '|').replace('/', '|').split('|')\n",
    "\n",
    "    first_match = re.match(r'([а-яА-ЯA-Za-z\\-\\s]*([,и\\/]|или)\\s)', a)\n",
    "\n",
    "    if first_match:\n",
    "        first_match = first_match.group()\n",
    "        a = repl(a.replace(first_match, ''))\n",
    "\n",
    "        for i in a:\n",
    "            if len(i.split()) > 4:\n",
    "                return repl(main_skill)\n",
    "\n",
    "        variants = first_match.replace(',', ' ').replace(\n",
    "            ' или ', ' ').replace(' и ', ' ').replace('/', ' ')\n",
    "        variants = list(filter(len, variants.split(' ')))\n",
    "        if len(variants) > 1:\n",
    "            main_phrase = variants[:-1]\n",
    "            a.append(variants[-1])\n",
    "\n",
    "            skills = []\n",
    "\n",
    "            for i in a:\n",
    "                skills.append(\" \".join(main_phrase + [i]))\n",
    "            return skills\n",
    "        else:\n",
    "            return a + variants\n",
    "    else:\n",
    "        return [a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def split_into_skills(text: str) -> List[str]:\n",
    "    def _split_into_skills(x):\n",
    "        x = re.sub(r'([А-ЯA-Z])', r'\\n\\1', text)\n",
    "        return re.split(r'[\\n\\.,]', x)\n",
    "\n",
    "    pre_skills = list(nonempty(_split_into_skills(text)))\n",
    "    done_skills = []\n",
    "\n",
    "    for skill in pre_skills:\n",
    "        skill = skill.replace('/', ' / ').replace(\"\\xa0\", \" \")\n",
    "        if len(re.findall(\"\\([а-яА-ЯA-Za-z\\-\\s]*\\)\", skill)) > 0:\n",
    "            skill1 = re.findall(r'\\([а-яА-ЯA-Za-z\\-\\s]*\\)', skill)[0]\n",
    "            skill2 = re.findall(r'[а-яА-ЯA-Za-z\\-\\s]*\\(', skill)[0][:-1]\n",
    "            done_skills.extend([skill1, skill2])\n",
    "        elif len(re.findall(\"([а-яА-ЯA-Za-z\\-\\s]*([и\\/]|или)\\s[а-яА-ЯA-Za-z\\-])\", skill)) > 0:\n",
    "            new_skills = split_skill(skill)\n",
    "            if new_skills:\n",
    "                done_skills.extend(new_skills)\n",
    "\n",
    "        else:\n",
    "            done_skills.append(skill)\n",
    "    return done_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:58.333245Z",
     "start_time": "2018-02-13T11:20:58.323514Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "from joblib import delayed, Parallel\n",
    "\n",
    "def parallel(f, data):\n",
    "    \"\"\"Run parallel your func on all CPU\"\"\"\n",
    "    \n",
    "    return Parallel(n_jobs=-1, verbose=3, max_nbytes='1G')(delayed(f)(x) for x in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:58.520628Z",
     "start_time": "2018-02-13T11:20:58.493421Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_resume_vacancy() -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"Loads JSON data of SuperJob\"\"\"\n",
    "\n",
    "    def load_json_by_lines(file):\n",
    "        tmp = []\n",
    "        \n",
    "        with open(file) as f:\n",
    "            for line in tqdm(f.read().split(\"\\n\")):\n",
    "                if len(line) > 0:\n",
    "                    tmp.append(json.loads(line))\n",
    "        return tmp\n",
    "    \n",
    "    resume = load_json_by_lines(\"data/resume.json\")\n",
    "    vacancy = load_json_by_lines(\"data/vacancy.json\")\n",
    "    \n",
    "    return resume, vacancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T21:55:26.226797Z",
     "start_time": "2018-02-12T21:52:46.488011Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "resume, vacancy = load_resume_vacancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T22:11:54.920831Z",
     "start_time": "2018-02-12T22:11:18.533864Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(resume, open(\"resume.pck\", \"wb\"))\n",
    "pickle.dump(vacancy, open(\"vacancy.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:21:59.324689Z",
     "start_time": "2018-02-13T11:21:21.838668Z"
    }
   },
   "outputs": [],
   "source": [
    "resume = pickle.load(open(\"resume.pck\", \"rb\"))\n",
    "vacancy = pickle.load(open(\"vacancy.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_skills(vac):\n",
    "    vac = vac.copy()\n",
    "    skills = split_into_skills(vac['candidat'])\n",
    "\n",
    "    clear_skills, dirty_skills = [], []\n",
    "    \n",
    "    for skill in skills:\n",
    "        normalized = normalize_skill(skill)\n",
    "        \n",
    "        if len(normalized) > 0:\n",
    "            clear_skills.append(\" \".join(normalized[0]))\n",
    "            dirty_skills.append(\" \".join(normalized[1]))\n",
    "        \n",
    "    vac['clear_skills'] = clear_skills\n",
    "    vac['dirty_skills'] = dirty_skills\n",
    "    \n",
    "    return vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for vac in tqdm(vacancy):\n",
    "    data.append(add_skills(vac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(data, open(\"vacancy_with_skills.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"vacancy_with_skills.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = defaultdict(lambda: [])\n",
    "dirty_to_normal = {}\n",
    "\n",
    "for d in data:\n",
    "    themes[d['profession_tree_name']].extend(d['dirty_skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = []\n",
    "\n",
    "for i in resume:\n",
    "    works.extend(i['work_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for work in tqdm(works):\n",
    "    work_words = list(filter(lambda x: x not in stopwords, get_words(remove_numbers(work['work'].lower()))))\n",
    "    words |= set(work_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vac in tqdm(vacancy):\n",
    "    vac_words = list(filter(lambda x: x not in stopwords, get_words(remove_numbers(vac['work'].lower()))))\n",
    "    words |= set(vac_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(words, open(\"words.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open(\"words.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"ruwikiruscorpora_upos_cbow_300_20_2017.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_vector = np.array([0] * 300, dtype=np.float32)\n",
    "\n",
    "def _word2vec(word):\n",
    "    for i in [\"_NOUN\", \"_ADJ\", \"_VERB\"]:\n",
    "        tmp = \"{}{}\".format(word, i)\n",
    "        \n",
    "        if tmp in model:\n",
    "            return model[tmp]\n",
    "        else:\n",
    "            return tmp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cahce = {}\n",
    "\n",
    "for word in tqdm(words):\n",
    "    normal_form = morph.parse(word)[0].normal_form\n",
    "    cahce[word] = _word2vec(normal_form)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(cahce, open(\"cahce.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cahce = pickle.load(open(\"cahce.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for work in tqdm(works):\n",
    "    words = list(filter(lambda x: x not in stopwords, get_words(remove_numbers(work['work'].lower()))))\n",
    "    work['vec'] = np.mean([cahce[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(works, open(\"works_with_vecs.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = pickle.load(open(\"works_with_vecs.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vac in tqdm(vacancy):\n",
    "    words = list(filter(lambda x: x not in stopwords, get_words(remove_numbers(vac['work'].lower()))))\n",
    "    vac['vec'] = np.mean([cahce[word] for word in words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(vacancy, open(\"vacancy_with_vecs.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy = pickle.load(open(\"vacancy_with_vecs.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = defaultdict(lambda: {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_vectors = []\n",
    "normal_works = []\n",
    "\n",
    "for work in tqdm(works):\n",
    "    try:\n",
    "        work_vectors.append(list(work['vec']))\n",
    "        normal_works.append(work)\n",
    "    except:\n",
    "         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_vectors = np.asarray(work_vectors)\n",
    "now_matrix = np.asarray([vacancy[0]['vec'] for i in range(len(work_vectors))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can find the C code in this temporary file: /tmp/theano_compilation_error_y07t_vdp\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Compilation failed (return status=1): /usr/bin/ld: /nix/store/f111ij1fc83965m48bf2zqgiaq88fqv5-glibc-2.25/lib/../lib64/crti.o: unrecognized relocation (0x2a) in section `.init'. /usr/bin/ld: final link failed: Bad value. collect2: error: ld returned 1 exit status. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/gof/lazylinker_c.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazylinker_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/gof/lazylinker_c.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazylinker_ext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2d3ee711e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     object2, utils)\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m from theano.compile import (\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mSymbolicInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mSymbolicOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/compile/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/compile/mode.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/gof/vm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcxx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lazylinker will not be imported if theano.config.cxx is not set.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazylinker_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mCVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazylinker_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLazyLinker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/gof/lazylinker_c.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCC_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             cmodule.GCC_compiler.compile_str(dirname, code, location=loc,\n\u001b[0;32m--> 127\u001b[0;31m                                              preargs=args)\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;31m# Save version into the __init__.py file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0minit_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__init__.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/lib/python3.6/site-packages/theano/gof/cmodule.py\u001b[0m in \u001b[0;36mcompile_str\u001b[0;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;31m# difficult to read.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             raise Exception('Compilation failed (return status=%s): %s' %\n\u001b[0;32m-> 2362\u001b[0;31m                             (status, compile_stderr.replace('\\n', '. ')))\n\u001b[0m\u001b[1;32m   2363\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompilation_warning\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompile_stderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m             \u001b[0;31m# Print errors just below the command line.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Compilation failed (return status=1): /usr/bin/ld: /nix/store/f111ij1fc83965m48bf2zqgiaq88fqv5-glibc-2.25/lib/../lib64/crti.o: unrecognized relocation (0x2a) in section `.init'. /usr/bin/ld: final link failed: Bad value. collect2: error: ld returned 1 exit status. "
     ]
    }
   ],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_cos_sim_theano():\n",
    "    v1 = theano.tensor.vector(dtype='float32')\n",
    "    v2 = theano.tensor.vector(dtype='float32')\n",
    "    \n",
    "    numerator = theano.tensor.sum(v1*v2)\n",
    "    denominator = theano.tensor.sqrt(theano.tensor.sum(v1**2)*theano.tensor.sum(v2**2))\n",
    "   \n",
    "    return theano.function([v1, v2], numerator/denominator)\n",
    "\n",
    "cos_sim_theano_fn = compile_cos_sim_theano()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_vectors.shape, now_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pairwise_distances(work_vectors, now_matrix, metric='cosine', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([vacancy[0]['vec'], vacancy[1]['vec']], [works[0]['vec'], works[0]['vec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vac in tqdm(vacancy):\n",
    "    vectors = []\n",
    "    for work in tqdm(works):\n",
    "        answer[vac['id']][work['name']] = 1 - spatial.distance.cosine(work['vec'], vac['vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(works, open(\"vac_with_vecs.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works[0]['work'], works[1]['work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - spatial.distance.cosine(works[0]['vec'], works[1]['vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works[0]['vec'], works[1]['vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(work):      \n",
    "#     print(work)\n",
    "#     clear_skills, dirty_skills = [], []\n",
    "    \n",
    "#     for skill in split_into_skills(work['work']):\n",
    "#         normalized = normalize_skill(skill)\n",
    "        \n",
    "#         if len(normalized) > 0:\n",
    "#             clear_skills.append(\" \".join(normalized[0]))\n",
    "#             dirty_skills.append(\" \".join(normalized[1]))\n",
    "        \n",
    "#     return {\"name\": work['name'], \"clear_skills\": clear_skills, \"dirty_skills\": dirty_skills} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works_clear = parallel(f, works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(i):\n",
    "#     tmp = []\n",
    "#     for j in split_into_skills(i['work']):\n",
    "#         for g in get_words(j.lower()):\n",
    "#             tmp.append(g)\n",
    "#     return tmp\n",
    "# words = parallel(f, works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_only = []\n",
    "# for word in words:\n",
    "#     for k in word:\n",
    "#         words_only.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_only = list(set(words_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = {}\n",
    "\n",
    "# for word in tqdm(words_only):\n",
    "#     cache[word] = morph.parse(word)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_words(works[0][\"work\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for work in tqdm(works):\n",
    "#     text = work[\"work\"]\n",
    "#     text = text.lower()\n",
    "#     text = remove_numbers(text)\n",
    "#     text = split_into_skills(text)\n",
    "    \n",
    "#     for i in range(len(text)):\n",
    "#         text[i] = get_words(text[i])\n",
    "        \n",
    "#         def g(x):\n",
    "#             if x in cache:\n",
    "#                 return cache[x].normal_form\n",
    "#             else:\n",
    "#                 print(x)\n",
    "#                 return None\n",
    "#         text[i] = list(filter(lambda x: x, map(g, text[i])))\n",
    "        \n",
    "#     work[\"work_clear\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1354.6px",
    "right": "20px",
    "top": "386px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
