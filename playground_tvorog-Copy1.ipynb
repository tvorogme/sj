{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:56.341291Z",
     "start_time": "2018-02-13T11:20:56.338194Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple, Sequence\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from cytoolz import pipe\n",
    "from collections import Counter\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:56.919774Z",
     "start_time": "2018-02-13T11:20:56.574584Z"
    },
    "code_folding": [
     27
    ]
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stopwords = nltk.corpus.stopwords.words(\n",
    "    'russian') + nltk.corpus.stopwords.words('english')\n",
    "stopwords += [\n",
    "    'отличный', 'метр', 'наш', 'клиент', 'банка', 'проект', 'литр',\n",
    "    'желательный', 'др', 'самый', 'мочь', 'хороший', 'год', 'чел', 'обязательный'\n",
    "]\n",
    "\n",
    "cache = {}\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def _get_POS(word):\n",
    "    if word not in cache:\n",
    "        cache[word] = morph.parse(word)[0]\n",
    "    return cache[word].tag.POS\n",
    "\n",
    "\n",
    "def normal_form(word):\n",
    "    if word not in cache:\n",
    "        cache[word] = morph.parse(word)[0]\n",
    "    return cache[word].normal_form\n",
    "\n",
    "\n",
    "def is_word_pos_in(word: str, pos: List[str] = None) -> bool:\n",
    "    if not pos:\n",
    "        pos = ['NOUN', \"ADJF\", 'INFN', 'VERB', 'ADJS']\n",
    "\n",
    "    return _get_POS(word) in pos\n",
    "\n",
    "\n",
    "def get_words(text):\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "\n",
    "def nonempty(x):\n",
    "    if isinstance(x, Sequence):\n",
    "        return filter(lambda x: len(x) > 0 and x != ' ', x)\n",
    "    return x\n",
    "\n",
    "helper = {}\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def normalize_skill(skill: str):\n",
    "    parsed = tuple(\n",
    "        pipe(\n",
    "            skill,\n",
    "            lambda x: x.lower(),\n",
    "            remove_numbers,\n",
    "            get_words,\n",
    "        ))\n",
    "    \n",
    "    clear_skill = []\n",
    "    dirty_skill = []\n",
    "    \n",
    "    # Последнее стоп слово для dirty_skill\n",
    "    last_stopword = None\n",
    "    \n",
    "    # Для каждого слова в скилле\n",
    "    for i in parsed:\n",
    "        # Нормализуем слово\n",
    "        word = normal_form(i)\n",
    "        \n",
    "        # Если стоп слово - запомним его\n",
    "        if word in nltk.corpus.stopwords.words('russian'):\n",
    "            last_stopword = word\n",
    "            \n",
    "            if word == \"без\":\n",
    "                clear_skill.append(word)\n",
    "        \n",
    "        # Проверим на часть речи, длинну и стоплова\n",
    "        elif is_word_pos_in(word) and len(word) > 3 and word not in stopwords:\n",
    "            \n",
    "            # Если до этого было стоп слово, добавим его в dirty\n",
    "            if last_stopword and len(dirty_skill) > 0:\n",
    "                dirty_skill.append(last_stopword)\n",
    "                last_stopword = None\n",
    "            \n",
    "            # Добавим в чистый скилл слово\n",
    "            clear_skill.append(word)\n",
    "            \n",
    "            if is_word_pos_in(word, ['NOUN', 'ADJF']):\n",
    "                dirty_skill.append(i)\n",
    "\n",
    "    if len(clear_skill) > 1 and len(clear_skill) < 8:\n",
    "        return clear_skill, dirty_skill\n",
    "    \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_skill(a):\n",
    "    '''Рассказ о программистах или о жизни -> рассказ о программистах, рассказ о жизни'''\n",
    "    main_skill = a\n",
    "\n",
    "    def repl(x): return x.replace(',', '|').replace(\n",
    "        ' или ', '|').replace(' и ', '|').replace('/', '|').split('|')\n",
    "\n",
    "    first_match = re.match(r'([а-яА-ЯA-Za-z\\-\\s]*([,и\\/]|или)\\s)', a)\n",
    "\n",
    "    if first_match:\n",
    "        first_match = first_match.group()\n",
    "        a = repl(a.replace(first_match, ''))\n",
    "\n",
    "        for i in a:\n",
    "            if len(i.split()) > 4:\n",
    "                return repl(main_skill)\n",
    "\n",
    "        variants = first_match.replace(',', ' ').replace(\n",
    "            ' или ', ' ').replace(' и ', ' ').replace('/', ' ')\n",
    "        variants = list(filter(len, variants.split(' ')))\n",
    "        if len(variants) > 1:\n",
    "            main_phrase = variants[:-1]\n",
    "            a.append(variants[-1])\n",
    "\n",
    "            skills = []\n",
    "\n",
    "            for i in a:\n",
    "                skills.append(\" \".join(main_phrase + [i]))\n",
    "            return skills\n",
    "        else:\n",
    "            return a + variants\n",
    "    else:\n",
    "        return [a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_into_skills(text: str) -> List[str]:\n",
    "    def _split_into_skills(x):\n",
    "        x = re.sub(r'([А-ЯA-Z])', r'\\n\\1', text)\n",
    "        return re.split(r'[\\n\\.,]', x)\n",
    "\n",
    "    pre_skills = list(nonempty(_split_into_skills(text)))\n",
    "    done_skills = []\n",
    "\n",
    "    for skill in pre_skills:\n",
    "        skill = skill.replace('/', ' / ').replace(\"\\xa0\", \" \")\n",
    "        if len(re.findall(\"\\([а-яА-ЯA-Za-z\\-\\s]*\\)\", skill)) > 0:\n",
    "            skill1 = re.findall(r'\\([а-яА-ЯA-Za-z\\-\\s]*\\)', skill)[0]\n",
    "            skill2 = re.findall(r'[а-яА-ЯA-Za-z\\-\\s]*\\(', skill)[0][:-1]\n",
    "            done_skills.extend([skill1, skill2])\n",
    "        elif len(re.findall(\"([а-яА-ЯA-Za-z\\-\\s]*([и\\/]|или)\\s[а-яА-ЯA-Za-z\\-])\", skill)) > 0:\n",
    "            new_skills = split_skill(skill)\n",
    "            if new_skills:\n",
    "                done_skills.extend(new_skills)\n",
    "\n",
    "        else:\n",
    "            done_skills.append(skill)\n",
    "    return done_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:58.333245Z",
     "start_time": "2018-02-13T11:20:58.323514Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import delayed, Parallel\n",
    "\n",
    "def parallel(f, data):\n",
    "    \"\"\"Run parallel your func on all CPU\"\"\"\n",
    "    \n",
    "    return Parallel(n_jobs=-1, verbose=3, max_nbytes='1G')(delayed(f)(x) for x in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:20:58.520628Z",
     "start_time": "2018-02-13T11:20:58.493421Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_resume_vacancy() -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"Loads JSON data of SuperJob\"\"\"\n",
    "\n",
    "    def load_json_by_lines(file):\n",
    "        tmp = []\n",
    "        \n",
    "        with open(file) as f:\n",
    "            for line in tqdm(f.read().split(\"\\n\")):\n",
    "                if len(line) > 0:\n",
    "                    tmp.append(json.loads(line))\n",
    "        return tmp\n",
    "    \n",
    "    resume = load_json_by_lines(\"data/resume.json\")\n",
    "    vacancy = load_json_by_lines(\"data/vacancy.json\")\n",
    "    \n",
    "    return resume, vacancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T21:55:26.226797Z",
     "start_time": "2018-02-12T21:52:46.488011Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "resume, vacancy = load_resume_vacancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T22:11:54.920831Z",
     "start_time": "2018-02-12T22:11:18.533864Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(resume, open(\"resume.pck\", \"wb\"))\n",
    "pickle.dump(vacancy, open(\"vacancy.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T11:21:59.324689Z",
     "start_time": "2018-02-13T11:21:21.838668Z"
    }
   },
   "outputs": [],
   "source": [
    "resume = pickle.load(open(\"resume.pck\", \"rb\"))\n",
    "vacancy = pickle.load(open(\"vacancy.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_skills(vac):\n",
    "    vac = vac.copy()\n",
    "    skills = split_into_skills(vac['candidat'])\n",
    "\n",
    "    clear_skills, dirty_skills = [], []\n",
    "    \n",
    "    for skill in skills:\n",
    "        normalized = normalize_skill(skill)\n",
    "        \n",
    "        if len(normalized) > 0:\n",
    "            clear_skills.append(\" \".join(normalized[0]))\n",
    "            dirty_skills.append(\" \".join(normalized[1]))\n",
    "        \n",
    "    vac['clear_skills'] = clear_skills\n",
    "    vac['dirty_skills'] = dirty_skills\n",
    "    \n",
    "    return vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for vac in tqdm(vacancy):\n",
    "    data.append(add_skills(vac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(data, open(\"vacancy_with_skills.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"vacancy_with_skills.pck\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = defaultdict(lambda: [])\n",
    "dirty_to_normal = {}\n",
    "\n",
    "for d in data:\n",
    "    themes[d['profession_tree_name']].extend(d['dirty_skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = []\n",
    "\n",
    "for i in resume:\n",
    "    works.extend(i['work_history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(work):      \n",
    "#     print(work)\n",
    "#     clear_skills, dirty_skills = [], []\n",
    "    \n",
    "#     for skill in split_into_skills(work['work']):\n",
    "#         normalized = normalize_skill(skill)\n",
    "        \n",
    "#         if len(normalized) > 0:\n",
    "#             clear_skills.append(\" \".join(normalized[0]))\n",
    "#             dirty_skills.append(\" \".join(normalized[1]))\n",
    "        \n",
    "#     return {\"name\": work['name'], \"clear_skills\": clear_skills, \"dirty_skills\": dirty_skills} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works_clear = parallel(f, works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(i):\n",
    "#     tmp = []\n",
    "#     for j in split_into_skills(i['work']):\n",
    "#         for g in get_words(j.lower()):\n",
    "#             tmp.append(g)\n",
    "#     return tmp\n",
    "# words = parallel(f, works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_only = []\n",
    "# for word in words:\n",
    "#     for k in word:\n",
    "#         words_only.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_only = list(set(words_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = {}\n",
    "\n",
    "# for word in tqdm(words_only):\n",
    "#     cache[word] = morph.parse(word)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_words(works[0][\"work\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for work in tqdm(works):\n",
    "#     text = work[\"work\"]\n",
    "#     text = text.lower()\n",
    "#     text = remove_numbers(text)\n",
    "#     text = split_into_skills(text)\n",
    "    \n",
    "#     for i in range(len(text)):\n",
    "#         text[i] = get_words(text[i])\n",
    "        \n",
    "#         def g(x):\n",
    "#             if x in cache:\n",
    "#                 return cache[x].normal_form\n",
    "#             else:\n",
    "#                 print(x)\n",
    "#                 return None\n",
    "#         text[i] = list(filter(lambda x: x, map(g, text[i])))\n",
    "        \n",
    "#     work[\"work_clear\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1354.6px",
    "right": "20px",
    "top": "386px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
